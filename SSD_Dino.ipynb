{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSD_Dino.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ik79fhVEOXJ",
        "colab_type": "code",
        "outputId": "9b949645-ff18-470c-c9e0-a14920e75b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4p2trcjERRZ",
        "colab_type": "code",
        "outputId": "a66d2e9c-8951-47a0-b730-f1f7fc394211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('Gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at Gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG8M66D1EbBI",
        "colab_type": "code",
        "outputId": "0c895bb5-c9fb-4971-d6ae-652271b2d561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "%cd /content/Gdrive/My Drive\n",
        "\n",
        "if not os.path.exists('Tensorflow'):\n",
        "  os.makedirs('Tensorflow')\n",
        "\n",
        "%cd /content/Gdrive/My Drive/Tensorflow\n",
        "\n",
        "#tensorflow models section has an object detection library\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "\n",
        "#this reposity has what it takes to prepare tfrecord files, bing thanks to Tony608\n",
        "!git clone --quiet https://github.com/Tony607/object_detection_demo\n",
        " \n",
        "#install PyDrive so we can upload our dataset from Google Drive\n",
        "!pip install PyDrive\n",
        "\n",
        "# protobuf is needed for creating py files from models library above\n",
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "\n",
        "# !pip install -q Cython contextlib2 pillow lxml matplotlib\n",
        "\n",
        "# pycoco for coco scores\n",
        "!pip install -q pycocotools\n",
        "\n",
        "# creating py files from protos\n",
        "%cd /content/Gdrive/My Drive/Tensorflow/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "# setting path, if not set, python can not use object detection library (from models)\n",
        "os.environ['PYTHONPATH'] += ':/content/Gdrive/My Drive/Tensorflow/models/research/:/content/Gdrive/My Drive/Tensorflow/models/research/slim/'\n",
        "\n",
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Gdrive/My Drive\n",
            "/content/Gdrive/My Drive/Tensorflow\n",
            "fatal: destination path 'models' already exists and is not an empty directory.\n",
            "fatal: destination path 'object_detection_demo' already exists and is not an empty directory.\n",
            "Collecting PyDrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K     |████████████████████████████████| 993kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.9)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.4.2)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.5)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n",
            "Building wheels for collected packages: PyDrive\n",
            "  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built PyDrive\n",
            "Installing collected packages: PyDrive\n",
            "Successfully installed PyDrive-1.3.1\n",
            "Selecting previously unselected package python-bs4.\n",
            "(Reading database ... 130963 files and directories currently installed.)\n",
            "Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n",
            "Unpacking python-bs4 (4.6.0-1) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-chardet.\n",
            "Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n",
            "Unpacking python-chardet (3.0.4-1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-webencodings.\n",
            "Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n",
            "Unpacking python-webencodings (0.5-2) ...\n",
            "Selecting previously unselected package python-html5lib.\n",
            "Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n",
            "Unpacking python-html5lib (0.999999999-1) ...\n",
            "Selecting previously unselected package python-lxml:amd64.\n",
            "Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python-olefile.\n",
            "Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n",
            "Unpacking python-olefile (0.45.1-1) ...\n",
            "Selecting previously unselected package python-pil:amd64.\n",
            "Preparing to unpack .../8-python-pil_5.1.0-1_amd64.deb ...\n",
            "Unpacking python-pil:amd64 (5.1.0-1) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python-bs4 (4.6.0-1) ...\n",
            "Setting up python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Setting up python-olefile (0.45.1-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up python-pil:amd64 (5.1.0-1) ...\n",
            "Setting up python-webencodings (0.5-2) ...\n",
            "Setting up python-chardet (3.0.4-1) ...\n",
            "Setting up python-html5lib (0.999999999-1) ...\n",
            "/content/Gdrive/My Drive/Tensorflow/models/research\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0714 01:55:41.279983 140084218398592 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0714 01:55:58.613656 140084218398592 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0714 01:56:08.842673 140084218398592 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Running tests under Python 3.6.8: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_models_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
            "[       OK ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
            "[       OK ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_model_config_proto\n",
            "[       OK ] ModelBuilderTest.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
            "[       OK ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTest.test_session\n",
            "[  SKIPPED ] ModelBuilderTest.test_session\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
            "[       OK ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_meta_architecture\n",
            "[       OK ] ModelBuilderTest.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
            "[       OK ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 16 tests in 0.134s\n",
            "\n",
            "OK (skipped=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2y7vfNSElH2",
        "colab_type": "code",
        "outputId": "5cc0019a-e4de-41dc-e20d-abec757d9581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "import os\n",
        "\n",
        "%cd /content/Gdrive/My Drive/Tensorflow\n",
        "if not os.path.exists('workspace'):\n",
        "  os.makedirs('workspace')\n",
        "\n",
        "\n",
        "%cd /content/Gdrive/My Drive/Tensorflow/workspace\n",
        "%rm -rf training_dino\n",
        "\n",
        "#####replace training_OPM1 with your training dir name#####\n",
        "if not os.path.exists('training_dino'):\n",
        "  os.makedirs('training_dino')\n",
        "\n",
        "training_dino = '/content/Gdrive/My\\ Drive/Tensorflow/workspace/training_dino'\n",
        "%cd {training_dino}\n",
        "\n",
        "\n",
        "fileId = '1y5J34231gI3ap0GUKc_fpMEsqAdXsE16'\n",
        "\n",
        "from zipfile import ZipFile\n",
        "from shutil import copy\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Create GoogleDrive instance with authenticated GoogleAuth instance.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Create GoogleDriveFile instance \n",
        "fileName = fileId + '.zip'\n",
        "downloaded = drive.CreateFile({'id': fileId})\n",
        "downloaded.GetContentFile(fileName)\n",
        "ds = ZipFile(fileName)\n",
        "ds.extractall()\n",
        "os.remove(fileName)\n",
        "print('Extracted zip file ' + fileName)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Gdrive/My Drive/Tensorflow\n",
            "/content/Gdrive/My Drive/Tensorflow/workspace\n",
            "/content/Gdrive/My Drive/Tensorflow/workspace/training_dino\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0713 12:48:06.190999 139745989588864 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    'file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth')\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracted zip file 1y5J34231gI3ap0GUKc_fpMEsqAdXsE16.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwz-7wyXFNdz",
        "colab_type": "code",
        "outputId": "f11604ff-1a74-443e-8b53-e369b612e9d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%cd {training_dino}\n",
        "\n",
        "# modified from https://www.dlology.com/blog/how-to-train-an-object-detection-model-easy-for-free/\n",
        "\n",
        "# Convert train folder annotation xml files to a single csv file,\n",
        "!python /content/Gdrive/My\\ Drive/Tensorflow/object_detection_demo/xml_to_csv.py \\\n",
        "-i train/Annotations -o train/train_labels.csv -l train/Annotations\n",
        "\n",
        "!python /content/Gdrive/My\\ Drive/Tensorflow/object_detection_demo/xml_to_csv.py \\\n",
        "-i test/Annotations -o test/test_labels.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Gdrive/My Drive/Tensorflow/workspace/training_dino\n",
            "Successfully converted xml to csv.\n",
            "Generate `train/Annotations/label_map.pbtxt`\n",
            "Successfully converted xml to csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W3rRnBhFhjZ",
        "colab_type": "code",
        "outputId": "4402455f-d177-425e-cbc5-8ddeaf2767cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "training_dino = '/content/Gdrive/My\\ Drive/Tensorflow/workspace/training_dino'\n",
        "%cd {training_dino}\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pd = pd.read_csv('train/train_labels.csv')\n",
        "classes = sorted(pd['class'].unique())\n",
        "\n",
        "#create pb file of label map\n",
        "#mapping of your classes with its corresponding identification number (id) in \n",
        "with open('label_map.pbtxt', 'w') as txtl:\n",
        "  for i, class_name in enumerate(classes):\n",
        "    txtl.write(\"item {\\n  id: %s\\n  name: '%s'\\n}\\n\\n\"%(i+1, class_name))\n",
        "\n",
        "with open('label_map.pbtxt') as txtl:\n",
        "  print(txtl.read())  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Gdrive/My Drive/Tensorflow/workspace/training_dino\n",
            "item {\n",
            "  id: 1\n",
            "  name: 'Bird'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 2\n",
            "  name: 'Cactus'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 3\n",
            "  name: 'Dino'\n",
            "}\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHVNM98oFuMi",
        "colab_type": "code",
        "outputId": "ca795335-9caf-43da-fb1b-52cd82e385f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "%cd {training_dino}\n",
        "%rm -rf tfrecord\n",
        "\n",
        "if not os.path.exists('tfrecord'):\n",
        "  os.makedirs('tfrecord')\n",
        "\n",
        "  \n",
        "# Generate `train.record`\n",
        "!python /content/Gdrive/My\\ Drive/Tensorflow/object_detection_demo/generate_tfrecord.py \\\n",
        "--csv_input=train/train_labels.csv \\\n",
        "--output_path=tfrecord/train.record \\\n",
        "--img_path=train/Images \\\n",
        "--label_map label_map.pbtxt\n",
        "\n",
        "# Generate `val.record`\n",
        "!python /content/Gdrive/My\\ Drive/Tensorflow/object_detection_demo/generate_tfrecord.py \\\n",
        "--csv_input=test/test_labels.csv \\\n",
        "--output_path=tfrecord/val.record \\\n",
        "--img_path=test/Images \\\n",
        "--label_map label_map.pbtxt\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Gdrive/My Drive/Tensorflow/workspace/training_dino\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0713 12:48:27.438616 139733450860416 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/object_detection_demo/generate_tfrecord.py:134: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0713 12:48:27.439424 139733450860416 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/object_detection_demo/generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0713 12:48:27.901043 139733450860416 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/utils/label_map_util.py:132: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/Gdrive/My Drive/Tensorflow/workspace/training_dino/tfrecord/train.record\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0713 12:48:32.104179 139815561107328 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/object_detection_demo/generate_tfrecord.py:134: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0713 12:48:32.104878 139815561107328 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/object_detection_demo/generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0713 12:48:32.129075 139815561107328 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/utils/label_map_util.py:132: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/Gdrive/My Drive/Tensorflow/workspace/training_dino/tfrecord/val.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UR2JdpeF2uA",
        "colab_type": "code",
        "outputId": "a8d2eb45-8bd5-4d6a-976f-b7dfdc5ea9f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd {training_dino}\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "from urllib.request import urlretrieve\n",
        "import tarfile\n",
        "\n",
        "\n",
        "\n",
        "#list of pretrained models can be found at:\n",
        "#https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\n",
        "MODEL = 'ssdlite_mobilenet_v2_coco_2018_05_09'\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = 'ssdlite_model'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "  urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "#remove zip file\n",
        "os.remove(MODEL_FILE)\n",
        "\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "  shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Gdrive/My Drive/Tensorflow/workspace/training_dino\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L0sHpDkGVNt",
        "colab_type": "code",
        "outputId": "e43e9623-8c7f-46f6-a4ce-abe99726538b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# modified from https://www.dlology.com/blog/how-to-train-an-object-detection-model-easy-for-free/\n",
        "\n",
        "%cd {training_dino}\n",
        "\n",
        "import re\n",
        "import os\n",
        "\n",
        "pipeline_fn = '/content/Gdrive/My Drive/Tensorflow/models/research/object_detection/samples/configs/ssdlite_mobilenet_v2_coco.config'\n",
        "checkpoint = os.path.join('.', 'ssdlite_model/model.ckpt')\n",
        "train_record = os.path.join('.', 'tfrecord/train.record')\n",
        "val_record = os.path.join('.', 'tfrecord/val.record')\n",
        "label_map = os.path.join('.', 'label_map.pbtxt')\n",
        "\n",
        "# generate examples by augmenting existing images in tfrecords\n",
        "num_eval = len(os.listdir('test/Images'))\n",
        "\n",
        "# specify classes\n",
        "num_class = len(classes)\n",
        "\n",
        "# more steps of training gives higher accuracy\n",
        "num_steps = 20000\n",
        "\n",
        "#this specific arguably help speed up training process as it fits in Colabe's Tesla K80 GPU memory for selected model.\n",
        "#I do not reccomend anything beyond size of 24(default) as I have tried, found training process significantly slower.\n",
        "batch_size = 12\n",
        "with open(pipeline_fn) as f:\n",
        "    s = f.read()\n",
        "    print(s)\n",
        "with open(pipeline_fn, 'w') as f:\n",
        "    \n",
        "    # fine_tune_checkpoint\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(checkpoint), s)\n",
        "    \n",
        "    # tfrecord files train and test.\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(val_record), s)\n",
        "\n",
        "    # label_map_path\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map), s)\n",
        "\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "    \n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_class), s)\n",
        "    \n",
        "    s = re.sub('num_examples: [0-9]+',\n",
        "               'num_examples: {}'.format(num_eval), s)\n",
        "    \n",
        "    f.write(s)\n",
        "    print(s)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Gdrive/My Drive/Tensorflow/workspace/training_dino\n",
            "# SSDLite with Mobilenet v2 configuration for MSCOCO Dataset.\n",
            "# Users should configure the fine_tune_checkpoint field in the train config as\n",
            "# well as the label_map_path and input_path fields in the train_input_reader and\n",
            "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
            "# should be configured.\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    num_classes: 3\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    anchor_generator {\n",
            "      ssd_anchor_generator {\n",
            "        num_layers: 6\n",
            "        min_scale: 0.2\n",
            "        max_scale: 0.95\n",
            "        aspect_ratios: 1.0\n",
            "        aspect_ratios: 2.0\n",
            "        aspect_ratios: 0.5\n",
            "        aspect_ratios: 3.0\n",
            "        aspect_ratios: 0.3333\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 300\n",
            "        width: 300\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      convolutional_box_predictor {\n",
            "        min_depth: 0\n",
            "        max_depth: 0\n",
            "        num_layers_before_predictor: 0\n",
            "        use_dropout: false\n",
            "        dropout_keep_probability: 0.8\n",
            "        kernel_size: 3\n",
            "        use_depthwise: true\n",
            "        box_code_size: 4\n",
            "        apply_sigmoid_to_scores: false\n",
            "        conv_hyperparams {\n",
            "          activation: RELU_6,\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.00004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            truncated_normal_initializer {\n",
            "              stddev: 0.03\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            train: true,\n",
            "            scale: true,\n",
            "            center: true,\n",
            "            decay: 0.9997,\n",
            "            epsilon: 0.001,\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_mobilenet_v2'\n",
            "      min_depth: 16\n",
            "      depth_multiplier: 1.0\n",
            "      use_depthwise: true\n",
            "      conv_hyperparams {\n",
            "        activation: RELU_6,\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.00004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            stddev: 0.03\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          train: true,\n",
            "          scale: true,\n",
            "          center: true,\n",
            "          decay: 0.9997,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid {\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      hard_example_miner {\n",
            "        num_hard_examples: 3000\n",
            "        iou_threshold: 0.99\n",
            "        loss_type: CLASSIFICATION\n",
            "        max_negatives_per_positive: 3\n",
            "        min_negatives_per_image: 3\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.6\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  batch_size: 12\n",
            "  optimizer {\n",
            "    rms_prop_optimizer: {\n",
            "      learning_rate: {\n",
            "        exponential_decay_learning_rate {\n",
            "          initial_learning_rate: 0.004\n",
            "          decay_steps: 800720\n",
            "          decay_factor: 0.95\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "      decay: 0.9\n",
            "      epsilon: 1.0\n",
            "    }\n",
            "  }\n",
            "  fine_tune_checkpoint: \"./ssdlite_model/model.ckpt\"\n",
            "  fine_tune_checkpoint_type:  \"detection\"\n",
            "  # Note: The below line limits the training process to 200K steps, which we\n",
            "  # empirically found to be sufficient enough to train the pets dataset. This\n",
            "  # effectively bypasses the learning rate schedule (the learning rate will\n",
            "  # never decay). Remove the below line to train indefinitely.\n",
            "  num_steps: 10000\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    ssd_random_crop {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"./tfrecord/train.record\"\n",
            "  }\n",
            "  label_map_path: \"./label_map.pbtxt\"\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  num_examples: 6\n",
            "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
            "  # Remove the below line to evaluate indefinitely.\n",
            "  max_evals: 10\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"./tfrecord/val.record\"\n",
            "  }\n",
            "  label_map_path: \"./label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_readers: 1\n",
            "}\n",
            "# SSDLite with Mobilenet v2 configuration for MSCOCO Dataset.\n",
            "# Users should configure the fine_tune_checkpoint field in the train config as\n",
            "# well as the label_map_path and input_path fields in the train_input_reader and\n",
            "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
            "# should be configured.\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    num_classes: 3\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    anchor_generator {\n",
            "      ssd_anchor_generator {\n",
            "        num_layers: 6\n",
            "        min_scale: 0.2\n",
            "        max_scale: 0.95\n",
            "        aspect_ratios: 1.0\n",
            "        aspect_ratios: 2.0\n",
            "        aspect_ratios: 0.5\n",
            "        aspect_ratios: 3.0\n",
            "        aspect_ratios: 0.3333\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 300\n",
            "        width: 300\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      convolutional_box_predictor {\n",
            "        min_depth: 0\n",
            "        max_depth: 0\n",
            "        num_layers_before_predictor: 0\n",
            "        use_dropout: false\n",
            "        dropout_keep_probability: 0.8\n",
            "        kernel_size: 3\n",
            "        use_depthwise: true\n",
            "        box_code_size: 4\n",
            "        apply_sigmoid_to_scores: false\n",
            "        conv_hyperparams {\n",
            "          activation: RELU_6,\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.00004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            truncated_normal_initializer {\n",
            "              stddev: 0.03\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            train: true,\n",
            "            scale: true,\n",
            "            center: true,\n",
            "            decay: 0.9997,\n",
            "            epsilon: 0.001,\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_mobilenet_v2'\n",
            "      min_depth: 16\n",
            "      depth_multiplier: 1.0\n",
            "      use_depthwise: true\n",
            "      conv_hyperparams {\n",
            "        activation: RELU_6,\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.00004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            stddev: 0.03\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          train: true,\n",
            "          scale: true,\n",
            "          center: true,\n",
            "          decay: 0.9997,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid {\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      hard_example_miner {\n",
            "        num_hard_examples: 3000\n",
            "        iou_threshold: 0.99\n",
            "        loss_type: CLASSIFICATION\n",
            "        max_negatives_per_positive: 3\n",
            "        min_negatives_per_image: 3\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.6\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  batch_size: 12\n",
            "  optimizer {\n",
            "    rms_prop_optimizer: {\n",
            "      learning_rate: {\n",
            "        exponential_decay_learning_rate {\n",
            "          initial_learning_rate: 0.004\n",
            "          decay_steps: 800720\n",
            "          decay_factor: 0.95\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "      decay: 0.9\n",
            "      epsilon: 1.0\n",
            "    }\n",
            "  }\n",
            "  fine_tune_checkpoint: \"./ssdlite_model/model.ckpt\"\n",
            "  fine_tune_checkpoint_type:  \"detection\"\n",
            "  # Note: The below line limits the training process to 200K steps, which we\n",
            "  # empirically found to be sufficient enough to train the pets dataset. This\n",
            "  # effectively bypasses the learning rate schedule (the learning rate will\n",
            "  # never decay). Remove the below line to train indefinitely.\n",
            "  num_steps: 20000\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    ssd_random_crop {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"./tfrecord/train.record\"\n",
            "  }\n",
            "  label_map_path: \"./label_map.pbtxt\"\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  num_examples: 6\n",
            "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
            "  # Remove the below line to evaluate indefinitely.\n",
            "  max_evals: 10\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"./tfrecord/val.record\"\n",
            "  }\n",
            "  label_map_path: \"./label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_readers: 1\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGgt_pcSHV02",
        "colab_type": "code",
        "outputId": "82c3f790-6528-4b94-e3c1-2e8183790986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd {training_dino}\n",
        "         \n",
        "\n",
        "pipeline_fn = '/content/Gdrive/My\\ Drive/Tensorflow/models/research/object_detection/samples/configs/ssdlite_mobilenet_v2_coco.config'\n",
        "obj_detection = '/content/Gdrive/My\\ Drive/Tensorflow/models/research/object_detection'\n",
        "\n",
        "!python {obj_detection + '/model_main.py'} \\\n",
        "    --pipeline_config_path={pipeline_fn}\\\n",
        "    --model_dir=fine_tuned_model \\\n",
        "    --alsologtostderr\\\n",
        " \n",
        "%ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Gdrive/My Drive/Tensorflow/workspace/training_dino\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0714 02:06:16.082246 140077666449280 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0714 02:06:17.056924 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0714 02:06:17.119185 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "W0714 02:06:23.149827 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/model_main.py:109: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0714 02:06:23.150769 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/utils/config_util.py:98: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0714 02:06:24.040987 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/model_lib.py:614: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "W0714 02:06:24.041177 140077666449280 model_lib.py:615] Forced number of epochs for all eval validations to be 1.\n",
            "W0714 02:06:24.041294 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/utils/config_util.py:484: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0714 02:06:24.041361 140077666449280 config_util.py:484] Maybe overwriting train_steps: None\n",
            "I0714 02:06:24.041425 140077666449280 config_util.py:484] Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "I0714 02:06:24.041496 140077666449280 config_util.py:484] Maybe overwriting use_bfloat16: False\n",
            "I0714 02:06:24.041558 140077666449280 config_util.py:484] Maybe overwriting eval_num_epochs: 1\n",
            "I0714 02:06:24.041619 140077666449280 config_util.py:484] Maybe overwriting load_pretrained: True\n",
            "I0714 02:06:24.041680 140077666449280 config_util.py:494] Ignoring config override key: load_pretrained\n",
            "W0714 02:06:24.041785 140077666449280 model_lib.py:631] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "I0714 02:06:24.041883 140077666449280 model_lib.py:666] create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
            "I0714 02:06:24.042441 140077666449280 estimator.py:209] Using config: {'_model_dir': 'fine_tuned_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6611481240>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "W0714 02:06:24.042674 140077666449280 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f661148da60>) includes params argument, but params are not passed to Estimator.\n",
            "I0714 02:06:24.043585 140077666449280 estimator_training.py:186] Not using Distribute Coordinator.\n",
            "I0714 02:06:24.043767 140077666449280 training.py:612] Running training and evaluation locally (non-distributed).\n",
            "I0714 02:06:24.044019 140077666449280 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "W0714 02:06:24.907186 140077666449280 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0714 02:06:24.918936 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/data_decoders/tf_example_decoder.py:177: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0714 02:06:24.919127 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/data_decoders/tf_example_decoder.py:192: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "W0714 02:06:25.152602 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0714 02:06:25.153962 140077666449280 dataset_builder.py:72] num_readers has been reduced to 1 to match input file shards.\n",
            "W0714 02:06:25.159545 140077666449280 deprecation.py:323] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0714 02:06:25.159686 140077666449280 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0714 02:06:25.188079 140077666449280 deprecation.py:323] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0714 02:06:25.352132 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/utils/ops.py:485: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
            "\n",
            "W0714 02:06:25.355991 140077666449280 deprecation.py:323] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/utils/ops.py:487: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0714 02:06:25.401687 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/core/preprocessor.py:512: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0714 02:06:25.454034 140077666449280 deprecation.py:323] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/core/preprocessor.py:188: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0714 02:06:26.288697 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/core/preprocessor.py:2515: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0714 02:06:26.918158 140077666449280 deprecation.py:323] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/builders/dataset_builder.py:158: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n",
            "I0714 02:06:26.930597 140077666449280 estimator.py:1145] Calling model_fn.\n",
            "I0714 02:06:30.684316 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:06:30.795817 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:06:30.914635 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:06:31.167222 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:06:31.278957 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:06:31.386242 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "W0714 02:06:31.500394 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/utils/variables_helper.py:134: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W0714 02:06:31.985710 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/model_lib.py:346: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
            "\n",
            "W0714 02:06:35.149739 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/meta_architectures/ssd_meta_arch.py:1066: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0714 02:06:35.155892 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/core/losses.py:172: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "W0714 02:06:35.157322 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/core/losses.py:178: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "W0714 02:06:35.492992 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/model_lib.py:373: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0714 02:06:35.493296 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/utils/learning_schedules.py:61: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "W0714 02:06:35.502734 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "W0714 02:06:37.890989 140077666449280 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "I0714 02:06:46.574982 140077666449280 estimator.py:1147] Done calling model_fn.\n",
            "I0714 02:06:46.576457 140077666449280 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0714 02:06:50.918232 140077666449280 monitored_session.py:240] Graph was finalized.\n",
            "2019-07-14 02:06:50.933184: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-07-14 02:06:50.934859: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2eb7100 executing computations on platform Host. Devices:\n",
            "2019-07-14 02:06:50.934905: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-07-14 02:06:50.941184: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-07-14 02:06:51.177291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:06:51.177777: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2eb6680 executing computations on platform CUDA. Devices:\n",
            "2019-07-14 02:06:51.177806: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-07-14 02:06:51.178064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:06:51.178411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-14 02:06:51.193200: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-14 02:06:51.366454: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-14 02:06:51.445542: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-14 02:06:51.468148: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-14 02:06:51.663126: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-14 02:06:51.770598: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-14 02:06:52.107270: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-14 02:06:52.107506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:06:52.108040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:06:52.108393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-14 02:06:52.113910: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-14 02:06:52.116369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-14 02:06:52.116404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-14 02:06:52.116420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-14 02:06:52.118885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:06:52.119330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:06:52.119665: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-07-14 02:06:52.119707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0714 02:06:52.122089 140077666449280 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0714 02:06:52.125013 140077666449280 saver.py:1280] Restoring parameters from fine_tuned_model/model.ckpt-14744\n",
            "W0714 02:06:56.551269 140077666449280 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "2019-07-14 02:06:57.614075: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "I0714 02:06:57.755898 140077666449280 session_manager.py:500] Running local_init_op.\n",
            "I0714 02:06:58.028577 140077666449280 session_manager.py:502] Done running local_init_op.\n",
            "I0714 02:07:09.335391 140077666449280 basic_session_run_hooks.py:606] Saving checkpoints for 14744 into fine_tuned_model/model.ckpt.\n",
            "2019-07-14 02:07:17.977146: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "I0714 02:07:22.178228 140077666449280 basic_session_run_hooks.py:262] loss = 0.9066436, step = 14744\n",
            "I0714 02:08:05.990257 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.28245\n",
            "I0714 02:08:05.991424 140077666449280 basic_session_run_hooks.py:260] loss = 1.2373366, step = 14844 (43.813 sec)\n",
            "I0714 02:08:45.866848 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.50774\n",
            "I0714 02:08:45.868007 140077666449280 basic_session_run_hooks.py:260] loss = 1.3547809, step = 14944 (39.877 sec)\n",
            "I0714 02:09:25.667736 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.51251\n",
            "I0714 02:09:25.669332 140077666449280 basic_session_run_hooks.py:260] loss = 0.9072472, step = 15044 (39.801 sec)\n",
            "I0714 02:10:05.930599 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.48368\n",
            "I0714 02:10:05.931614 140077666449280 basic_session_run_hooks.py:260] loss = 1.1775115, step = 15144 (40.262 sec)\n",
            "I0714 02:10:45.950686 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.49874\n",
            "I0714 02:10:45.951609 140077666449280 basic_session_run_hooks.py:260] loss = 0.9314847, step = 15244 (40.020 sec)\n",
            "I0714 02:11:26.209267 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.48394\n",
            "I0714 02:11:26.210896 140077666449280 basic_session_run_hooks.py:260] loss = 1.7124028, step = 15344 (40.259 sec)\n",
            "I0714 02:12:06.207293 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.50013\n",
            "I0714 02:12:06.208456 140077666449280 basic_session_run_hooks.py:260] loss = 0.9300523, step = 15444 (39.998 sec)\n",
            "I0714 02:12:46.205128 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.50013\n",
            "I0714 02:12:46.206297 140077666449280 basic_session_run_hooks.py:260] loss = 0.86289656, step = 15544 (39.998 sec)\n",
            "I0714 02:13:26.292741 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.49454\n",
            "I0714 02:13:26.294300 140077666449280 basic_session_run_hooks.py:260] loss = 1.2739847, step = 15644 (40.088 sec)\n",
            "I0714 02:14:06.218895 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.50463\n",
            "I0714 02:14:06.220035 140077666449280 basic_session_run_hooks.py:260] loss = 0.89159435, step = 15744 (39.926 sec)\n",
            "I0714 02:14:46.190617 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.50177\n",
            "I0714 02:14:46.192656 140077666449280 basic_session_run_hooks.py:260] loss = 1.1090504, step = 15844 (39.973 sec)\n",
            "I0714 02:15:26.004930 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.51166\n",
            "I0714 02:15:26.006033 140077666449280 basic_session_run_hooks.py:260] loss = 1.1649117, step = 15944 (39.813 sec)\n",
            "I0714 02:16:06.089903 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.4947\n",
            "I0714 02:16:06.091675 140077666449280 basic_session_run_hooks.py:260] loss = 0.9729879, step = 16044 (40.086 sec)\n",
            "I0714 02:16:45.930357 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.51001\n",
            "I0714 02:16:45.931516 140077666449280 basic_session_run_hooks.py:260] loss = 1.0902476, step = 16144 (39.840 sec)\n",
            "I0714 02:17:12.087404 140077666449280 basic_session_run_hooks.py:606] Saving checkpoints for 16211 into fine_tuned_model/model.ckpt.\n",
            "W0714 02:17:12.255845 140077666449280 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "I0714 02:17:14.541987 140077666449280 estimator.py:1145] Calling model_fn.\n",
            "I0714 02:17:17.455326 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:17:17.546497 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:17:17.647813 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:17:17.746888 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:17:17.839582 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:17:17.930383 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "W0714 02:17:18.838908 140077666449280 deprecation.py:323] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/eval_util.py:791: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0714 02:17:19.022643 140077666449280 deprecation.py:323] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/utils/visualization_utils.py:492: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0714 02:17:19.191637 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/utils/visualization_utils.py:1005: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "W0714 02:17:19.284669 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/model_lib.py:473: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
            "\n",
            "I0714 02:17:19.666767 140077666449280 estimator.py:1147] Done calling model_fn.\n",
            "I0714 02:17:19.686455 140077666449280 evaluation.py:255] Starting evaluation at 2019-07-14T02:17:19Z\n",
            "I0714 02:17:20.255563 140077666449280 monitored_session.py:240] Graph was finalized.\n",
            "2019-07-14 02:17:20.256834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:17:20.257184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-14 02:17:20.257282: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-14 02:17:20.257305: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-14 02:17:20.257338: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-14 02:17:20.257360: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-14 02:17:20.257377: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-14 02:17:20.257396: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-14 02:17:20.257416: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-14 02:17:20.257512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:17:20.257850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:17:20.258167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-14 02:17:20.258212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-14 02:17:20.258225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-14 02:17:20.258234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-14 02:17:20.258473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:17:20.258836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:17:20.259155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I0714 02:17:20.260930 140077666449280 saver.py:1280] Restoring parameters from fine_tuned_model/model.ckpt-16211\n",
            "I0714 02:17:21.280011 140077666449280 session_manager.py:500] Running local_init_op.\n",
            "I0714 02:17:21.381143 140077666449280 session_manager.py:502] Done running local_init_op.\n",
            "I0714 02:17:24.006168 140075811723008 coco_evaluation.py:200] Performing evaluation on 6 images.\n",
            "creating index...\n",
            "index created!\n",
            "I0714 02:17:24.006646 140075811723008 coco_tools.py:109] Loading and preparing annotation results...\n",
            "I0714 02:17:24.007318 140075811723008 coco_tools.py:131] DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.04s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.02s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.671\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.913\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.841\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.718\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.672\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.756\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.756\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.756\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            "I0714 02:17:24.178054 140077666449280 evaluation.py:275] Finished evaluation at 2019-07-14-02:17:24\n",
            "I0714 02:17:24.178293 140077666449280 estimator.py:2039] Saving dict for global step 16211: DetectionBoxes_Precision/mAP = 0.6706075, DetectionBoxes_Precision/mAP (large) = -1.0, DetectionBoxes_Precision/mAP (medium) = 0.7184779, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.91266286, DetectionBoxes_Precision/mAP@.75IOU = 0.841202, DetectionBoxes_Recall/AR@1 = 0.6722222, DetectionBoxes_Recall/AR@10 = 0.75555557, DetectionBoxes_Recall/AR@100 = 0.75555557, DetectionBoxes_Recall/AR@100 (large) = -1.0, DetectionBoxes_Recall/AR@100 (medium) = 0.75555557, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/classification_loss = 5.425138, Loss/localization_loss = 0.41118118, Loss/regularization_loss = 0.27393985, Loss/total_loss = 6.1102586, global_step = 16211, learning_rate = 0.004, loss = 6.1102586\n",
            "I0714 02:17:25.448085 140077666449280 estimator.py:2099] Saving 'checkpoint_path' summary for global step 16211: fine_tuned_model/model.ckpt-16211\n",
            "I0714 02:17:38.999214 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 1.88434\n",
            "I0714 02:17:39.000191 140077666449280 basic_session_run_hooks.py:260] loss = 1.2204425, step = 16244 (53.069 sec)\n",
            "I0714 02:18:18.647008 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.52221\n",
            "I0714 02:18:18.648380 140077666449280 basic_session_run_hooks.py:260] loss = 1.301756, step = 16344 (39.648 sec)\n",
            "I0714 02:18:58.188654 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.52898\n",
            "I0714 02:18:58.189630 140077666449280 basic_session_run_hooks.py:260] loss = 1.5894146, step = 16444 (39.541 sec)\n",
            "I0714 02:19:37.617214 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.53623\n",
            "I0714 02:19:37.618291 140077666449280 basic_session_run_hooks.py:260] loss = 1.6691661, step = 16544 (39.429 sec)\n",
            "I0714 02:20:17.005032 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.53886\n",
            "I0714 02:20:17.006297 140077666449280 basic_session_run_hooks.py:260] loss = 1.6067775, step = 16644 (39.388 sec)\n",
            "I0714 02:20:56.758772 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.51548\n",
            "I0714 02:20:56.760380 140077666449280 basic_session_run_hooks.py:260] loss = 1.687666, step = 16744 (39.754 sec)\n",
            "I0714 02:21:36.202265 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.53527\n",
            "I0714 02:21:36.203219 140077666449280 basic_session_run_hooks.py:260] loss = 0.6571535, step = 16844 (39.443 sec)\n",
            "I0714 02:22:15.750146 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.52858\n",
            "I0714 02:22:15.751195 140077666449280 basic_session_run_hooks.py:260] loss = 1.0055554, step = 16944 (39.548 sec)\n",
            "I0714 02:22:55.140043 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.53872\n",
            "I0714 02:22:55.140933 140077666449280 basic_session_run_hooks.py:260] loss = 0.6525878, step = 17044 (39.390 sec)\n",
            "I0714 02:23:34.772903 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.52316\n",
            "I0714 02:23:34.774420 140077666449280 basic_session_run_hooks.py:260] loss = 1.1388577, step = 17144 (39.634 sec)\n",
            "I0714 02:24:14.278339 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.5313\n",
            "I0714 02:24:14.279275 140077666449280 basic_session_run_hooks.py:260] loss = 1.3791336, step = 17244 (39.505 sec)\n",
            "I0714 02:24:53.523111 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.54811\n",
            "I0714 02:24:53.524114 140077666449280 basic_session_run_hooks.py:260] loss = 2.0030358, step = 17344 (39.245 sec)\n",
            "I0714 02:25:33.049709 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.52994\n",
            "I0714 02:25:33.050807 140077666449280 basic_session_run_hooks.py:260] loss = 1.1319455, step = 17444 (39.527 sec)\n",
            "I0714 02:26:12.971814 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.50488\n",
            "I0714 02:26:12.973781 140077666449280 basic_session_run_hooks.py:260] loss = 1.3290057, step = 17544 (39.923 sec)\n",
            "I0714 02:26:52.548814 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.52672\n",
            "I0714 02:26:52.549926 140077666449280 basic_session_run_hooks.py:260] loss = 1.0351863, step = 17644 (39.576 sec)\n",
            "I0714 02:27:12.291277 140077666449280 basic_session_run_hooks.py:606] Saving checkpoints for 17695 into fine_tuned_model/model.ckpt.\n",
            "I0714 02:27:14.709653 140077666449280 estimator.py:1145] Calling model_fn.\n",
            "I0714 02:27:17.269144 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:27:17.376053 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:27:17.480142 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:27:17.574442 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:27:17.669832 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:27:17.762490 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:27:19.514143 140077666449280 estimator.py:1147] Done calling model_fn.\n",
            "I0714 02:27:19.533434 140077666449280 evaluation.py:255] Starting evaluation at 2019-07-14T02:27:19Z\n",
            "I0714 02:27:20.708500 140077666449280 monitored_session.py:240] Graph was finalized.\n",
            "2019-07-14 02:27:20.709170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:27:20.709560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-14 02:27:20.709676: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-14 02:27:20.709699: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-14 02:27:20.709721: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-14 02:27:20.709741: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-14 02:27:20.709763: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-14 02:27:20.709782: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-14 02:27:20.709803: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-14 02:27:20.709938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:27:20.710390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:27:20.710713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-14 02:27:20.710763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-14 02:27:20.710778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-14 02:27:20.710790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-14 02:27:20.711084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:27:20.711480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:27:20.711789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I0714 02:27:20.713604 140077666449280 saver.py:1280] Restoring parameters from fine_tuned_model/model.ckpt-17695\n",
            "I0714 02:27:21.763967 140077666449280 session_manager.py:500] Running local_init_op.\n",
            "I0714 02:27:21.869177 140077666449280 session_manager.py:502] Done running local_init_op.\n",
            "I0714 02:27:23.921424 140075820115712 coco_evaluation.py:200] Performing evaluation on 6 images.\n",
            "creating index...\n",
            "index created!\n",
            "I0714 02:27:23.922906 140075820115712 coco_tools.py:109] Loading and preparing annotation results...\n",
            "I0714 02:27:23.923554 140075820115712 coco_tools.py:131] DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.04s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.02s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.719\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.992\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.992\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.724\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.728\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.728\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.728\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.728\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            "I0714 02:27:24.105828 140077666449280 evaluation.py:275] Finished evaluation at 2019-07-14-02:27:24\n",
            "I0714 02:27:24.106081 140077666449280 estimator.py:2039] Saving dict for global step 17695: DetectionBoxes_Precision/mAP = 0.7190358, DetectionBoxes_Precision/mAP (large) = -1.0, DetectionBoxes_Precision/mAP (medium) = 0.7238449, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.9919849, DetectionBoxes_Precision/mAP@.75IOU = 0.9919849, DetectionBoxes_Recall/AR@1 = 0.7277778, DetectionBoxes_Recall/AR@10 = 0.7277778, DetectionBoxes_Recall/AR@100 = 0.7277778, DetectionBoxes_Recall/AR@100 (large) = -1.0, DetectionBoxes_Recall/AR@100 (medium) = 0.7277778, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/classification_loss = 6.905997, Loss/localization_loss = 0.2514744, Loss/regularization_loss = 0.27396533, Loss/total_loss = 7.4314365, global_step = 17695, learning_rate = 0.004, loss = 7.4314365\n",
            "I0714 02:27:24.107993 140077666449280 estimator.py:2099] Saving 'checkpoint_path' summary for global step 17695: fine_tuned_model/model.ckpt-17695\n",
            "I0714 02:27:43.931122 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 1.94619\n",
            "I0714 02:27:43.932059 140077666449280 basic_session_run_hooks.py:260] loss = 0.7587229, step = 17744 (51.382 sec)\n",
            "I0714 02:28:23.352091 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.53672\n",
            "I0714 02:28:23.353497 140077666449280 basic_session_run_hooks.py:260] loss = 0.91373605, step = 17844 (39.421 sec)\n",
            "I0714 02:29:03.042418 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.5195\n",
            "I0714 02:29:03.043448 140077666449280 basic_session_run_hooks.py:260] loss = 1.949064, step = 17944 (39.690 sec)\n",
            "I0714 02:29:42.193811 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.55419\n",
            "I0714 02:29:42.195004 140077666449280 basic_session_run_hooks.py:260] loss = 1.4992397, step = 18044 (39.152 sec)\n",
            "I0714 02:30:21.543630 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.54131\n",
            "I0714 02:30:21.544798 140077666449280 basic_session_run_hooks.py:260] loss = 1.7211123, step = 18144 (39.350 sec)\n",
            "I0714 02:31:01.086762 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.52888\n",
            "I0714 02:31:01.088443 140077666449280 basic_session_run_hooks.py:260] loss = 1.226098, step = 18244 (39.544 sec)\n",
            "I0714 02:31:40.847123 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.51507\n",
            "I0714 02:31:40.848226 140077666449280 basic_session_run_hooks.py:260] loss = 1.2574494, step = 18344 (39.760 sec)\n",
            "I0714 02:32:20.420694 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.52694\n",
            "I0714 02:32:20.421699 140077666449280 basic_session_run_hooks.py:260] loss = 0.8939012, step = 18444 (39.573 sec)\n",
            "I0714 02:32:59.956284 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.52937\n",
            "I0714 02:32:59.957236 140077666449280 basic_session_run_hooks.py:260] loss = 1.1631767, step = 18544 (39.536 sec)\n",
            "I0714 02:33:39.281205 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.54291\n",
            "I0714 02:33:39.282720 140077666449280 basic_session_run_hooks.py:260] loss = 1.2263472, step = 18644 (39.325 sec)\n",
            "I0714 02:34:18.837789 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.52803\n",
            "I0714 02:34:18.838793 140077666449280 basic_session_run_hooks.py:260] loss = 0.8396216, step = 18744 (39.556 sec)\n",
            "I0714 02:34:58.012007 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.5527\n",
            "I0714 02:34:58.012938 140077666449280 basic_session_run_hooks.py:260] loss = 0.6141691, step = 18844 (39.174 sec)\n",
            "I0714 02:35:37.305659 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.54494\n",
            "I0714 02:35:37.306612 140077666449280 basic_session_run_hooks.py:260] loss = 1.1415946, step = 18944 (39.294 sec)\n",
            "I0714 02:36:16.804824 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.5317\n",
            "I0714 02:36:16.806373 140077666449280 basic_session_run_hooks.py:260] loss = 0.7561597, step = 19044 (39.500 sec)\n",
            "I0714 02:36:56.136230 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.5425\n",
            "I0714 02:36:56.137136 140077666449280 basic_session_run_hooks.py:260] loss = 1.3018788, step = 19144 (39.331 sec)\n",
            "I0714 02:37:12.647811 140077666449280 basic_session_run_hooks.py:606] Saving checkpoints for 19187 into fine_tuned_model/model.ckpt.\n",
            "I0714 02:37:15.084167 140077666449280 estimator.py:1145] Calling model_fn.\n",
            "I0714 02:37:17.597032 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:37:17.689544 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:37:17.790622 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:37:17.887829 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:37:17.979589 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:37:18.075308 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:37:20.401028 140077666449280 estimator.py:1147] Done calling model_fn.\n",
            "I0714 02:37:20.420576 140077666449280 evaluation.py:255] Starting evaluation at 2019-07-14T02:37:20Z\n",
            "I0714 02:37:20.970287 140077666449280 monitored_session.py:240] Graph was finalized.\n",
            "2019-07-14 02:37:20.970905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:37:20.971274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-14 02:37:20.971378: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-14 02:37:20.971414: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-14 02:37:20.971434: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-14 02:37:20.971455: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-14 02:37:20.971474: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-14 02:37:20.971493: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-14 02:37:20.971514: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-14 02:37:20.971615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:37:20.971989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:37:20.972254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-14 02:37:20.972295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-14 02:37:20.972307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-14 02:37:20.972316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-14 02:37:20.972537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:37:20.972873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:37:20.973160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I0714 02:37:20.975388 140077666449280 saver.py:1280] Restoring parameters from fine_tuned_model/model.ckpt-19187\n",
            "I0714 02:37:22.027618 140077666449280 session_manager.py:500] Running local_init_op.\n",
            "I0714 02:37:22.141482 140077666449280 session_manager.py:502] Done running local_init_op.\n",
            "I0714 02:37:24.202240 140075811723008 coco_evaluation.py:200] Performing evaluation on 6 images.\n",
            "creating index...\n",
            "index created!\n",
            "I0714 02:37:24.203564 140075811723008 coco_tools.py:109] Loading and preparing annotation results...\n",
            "I0714 02:37:24.204205 140075811723008 coco_tools.py:131] DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.03s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.02s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.803\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.992\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.992\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.809\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.811\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.811\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.811\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.811\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            "I0714 02:37:24.383545 140077666449280 evaluation.py:275] Finished evaluation at 2019-07-14-02:37:24\n",
            "I0714 02:37:24.383781 140077666449280 estimator.py:2039] Saving dict for global step 19187: DetectionBoxes_Precision/mAP = 0.8031023, DetectionBoxes_Precision/mAP (large) = -1.0, DetectionBoxes_Precision/mAP (medium) = 0.8087129, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.9919849, DetectionBoxes_Precision/mAP@.75IOU = 0.9919849, DetectionBoxes_Recall/AR@1 = 0.8111111, DetectionBoxes_Recall/AR@10 = 0.8111111, DetectionBoxes_Recall/AR@100 = 0.8111111, DetectionBoxes_Recall/AR@100 (large) = -1.0, DetectionBoxes_Recall/AR@100 (medium) = 0.8111111, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/classification_loss = 6.1849403, Loss/localization_loss = 0.19232191, Loss/regularization_loss = 0.27391806, Loss/total_loss = 6.6511807, global_step = 19187, learning_rate = 0.004, loss = 6.6511807\n",
            "I0714 02:37:24.385132 140077666449280 estimator.py:2099] Saving 'checkpoint_path' summary for global step 19187: fine_tuned_model/model.ckpt-19187\n",
            "I0714 02:37:47.320930 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 1.95371\n",
            "I0714 02:37:47.322367 140077666449280 basic_session_run_hooks.py:260] loss = 0.57873183, step = 19244 (51.185 sec)\n",
            "I0714 02:38:26.770129 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.5349\n",
            "I0714 02:38:26.771510 140077666449280 basic_session_run_hooks.py:260] loss = 1.1885698, step = 19344 (39.449 sec)\n",
            "I0714 02:39:06.058924 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.54526\n",
            "I0714 02:39:06.059876 140077666449280 basic_session_run_hooks.py:260] loss = 1.126103, step = 19444 (39.288 sec)\n",
            "I0714 02:39:45.561762 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.53146\n",
            "I0714 02:39:45.562952 140077666449280 basic_session_run_hooks.py:260] loss = 1.8201141, step = 19544 (39.503 sec)\n",
            "I0714 02:40:24.860119 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.54463\n",
            "I0714 02:40:24.861103 140077666449280 basic_session_run_hooks.py:260] loss = 0.99090815, step = 19644 (39.298 sec)\n",
            "I0714 02:41:04.749461 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.50694\n",
            "I0714 02:41:04.751062 140077666449280 basic_session_run_hooks.py:260] loss = 1.8718908, step = 19744 (39.890 sec)\n",
            "I0714 02:41:44.826523 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.49519\n",
            "I0714 02:41:44.827612 140077666449280 basic_session_run_hooks.py:260] loss = 1.4556923, step = 19844 (40.077 sec)\n",
            "I0714 02:42:25.019704 140077666449280 basic_session_run_hooks.py:692] global_step/sec: 2.48798\n",
            "I0714 02:42:25.020766 140077666449280 basic_session_run_hooks.py:260] loss = 1.1798711, step = 19944 (40.193 sec)\n",
            "I0714 02:42:47.134135 140077666449280 basic_session_run_hooks.py:606] Saving checkpoints for 20000 into fine_tuned_model/model.ckpt.\n",
            "I0714 02:42:48.925806 140077666449280 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "I0714 02:42:49.608122 140077666449280 estimator.py:1145] Calling model_fn.\n",
            "I0714 02:42:52.170962 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:42:52.263695 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:42:52.366394 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:42:52.462009 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:42:52.555091 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:42:52.647127 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:42:54.987362 140077666449280 estimator.py:1147] Done calling model_fn.\n",
            "I0714 02:42:55.006965 140077666449280 evaluation.py:255] Starting evaluation at 2019-07-14T02:42:55Z\n",
            "I0714 02:42:55.575087 140077666449280 monitored_session.py:240] Graph was finalized.\n",
            "2019-07-14 02:42:55.575772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:42:55.576152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-14 02:42:55.576272: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-14 02:42:55.576296: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-14 02:42:55.576328: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-14 02:42:55.576349: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-14 02:42:55.576371: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-14 02:42:55.576394: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-14 02:42:55.576419: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-14 02:42:55.576527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:42:55.576898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:42:55.577181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-14 02:42:55.577223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-14 02:42:55.577236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-14 02:42:55.577247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-14 02:42:55.577502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:42:55.577880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:42:55.578169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I0714 02:42:55.580104 140077666449280 saver.py:1280] Restoring parameters from fine_tuned_model/model.ckpt-20000\n",
            "I0714 02:42:56.671378 140077666449280 session_manager.py:500] Running local_init_op.\n",
            "I0714 02:42:56.797912 140077666449280 session_manager.py:502] Done running local_init_op.\n",
            "I0714 02:42:58.924045 140075820115712 coco_evaluation.py:200] Performing evaluation on 6 images.\n",
            "creating index...\n",
            "index created!\n",
            "I0714 02:42:58.924469 140075820115712 coco_tools.py:109] Loading and preparing annotation results...\n",
            "I0714 02:42:58.925039 140075820115712 coco_tools.py:131] DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.03s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.02s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.777\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.977\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.977\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.796\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.767\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.806\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.806\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.806\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            "I0714 02:42:59.099927 140077666449280 evaluation.py:275] Finished evaluation at 2019-07-14-02:42:59\n",
            "I0714 02:42:59.100185 140077666449280 estimator.py:2039] Saving dict for global step 20000: DetectionBoxes_Precision/mAP = 0.7772002, DetectionBoxes_Precision/mAP (large) = -1.0, DetectionBoxes_Precision/mAP (medium) = 0.7955776, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.97662264, DetectionBoxes_Precision/mAP@.75IOU = 0.97662264, DetectionBoxes_Recall/AR@1 = 0.76666665, DetectionBoxes_Recall/AR@10 = 0.8055556, DetectionBoxes_Recall/AR@100 = 0.8055556, DetectionBoxes_Recall/AR@100 (large) = -1.0, DetectionBoxes_Recall/AR@100 (medium) = 0.8055556, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/classification_loss = 7.9866943, Loss/localization_loss = 0.18898444, Loss/regularization_loss = 0.2738924, Loss/total_loss = 8.449571, global_step = 20000, learning_rate = 0.004, loss = 8.449571\n",
            "I0714 02:42:59.101605 140077666449280 estimator.py:2099] Saving 'checkpoint_path' summary for global step 20000: fine_tuned_model/model.ckpt-20000\n",
            "I0714 02:42:59.102436 140077666449280 exporter.py:410] Performing the final export in the end of training.\n",
            "I0714 02:42:59.320033 140077666449280 estimator.py:1145] Calling model_fn.\n",
            "I0714 02:43:01.873028 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:43:01.968044 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:43:02.072167 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:43:02.172602 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:43:02.264806 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:43:02.363651 140077666449280 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "W0714 02:43:02.873167 140077666449280 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/model_lib.py:419: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "I0714 02:43:03.772647 140077666449280 estimator.py:1147] Done calling model_fn.\n",
            "W0714 02:43:03.772920 140077666449280 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "I0714 02:43:03.773543 140077666449280 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
            "I0714 02:43:03.773639 140077666449280 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
            "I0714 02:43:03.773709 140077666449280 export_utils.py:170] Signatures INCLUDED in export for Predict: ['tensorflow/serving/predict', 'serving_default']\n",
            "I0714 02:43:03.773765 140077666449280 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
            "I0714 02:43:03.773820 140077666449280 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
            "2019-07-14 02:43:03.774343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:43:03.774708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-14 02:43:03.774790: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-14 02:43:03.774813: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-14 02:43:03.774834: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-14 02:43:03.774854: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-14 02:43:03.774893: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-14 02:43:03.774913: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-14 02:43:03.774934: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-14 02:43:03.775041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:43:03.775455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:43:03.775750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-14 02:43:03.775790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-14 02:43:03.775805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-14 02:43:03.775814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-14 02:43:03.776077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:43:03.776442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:43:03.776737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I0714 02:43:03.780282 140077666449280 saver.py:1280] Restoring parameters from fine_tuned_model/model.ckpt-20000\n",
            "I0714 02:43:04.332741 140077666449280 builder_impl.py:661] Assets added to graph.\n",
            "I0714 02:43:04.332973 140077666449280 builder_impl.py:456] No assets to write.\n",
            "I0714 02:43:05.228003 140077666449280 builder_impl.py:421] SavedModel written to: fine_tuned_model/export/Servo/temp-b'1563072179'/saved_model.pb\n",
            "I0714 02:43:05.635594 140077666449280 estimator.py:368] Loss for final step: 1.0807838.\n",
            "\u001b[0m\u001b[01;34mfine_tuned_model\u001b[0m/  \u001b[01;34m__MACOSX\u001b[0m/       \u001b[01;34mtest\u001b[0m/      \u001b[01;34mtrain\u001b[0m/\n",
            "label_map.pbtxt    \u001b[01;34mssdlite_model\u001b[0m/  \u001b[01;34mtfrecord\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsP3TkWSHoYH",
        "colab_type": "code",
        "outputId": "145fea32-3c63-4552-d2ce-29887b613783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "training_dino = '/content/Gdrive/My\\ Drive/Tensorflow/workspace/training_dino'\n",
        "obj_detection = '/content/Gdrive/My\\ Drive/Tensorflow/models/research/object_detection'\n",
        "\n",
        "%cd {training_dino}\n",
        "%rm -rf Frozen_Inf\n",
        "\n",
        "\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "#get the last model's check point\n",
        "ckpt_files = [ckpt.replace('.meta', '') for ckpt in os.listdir('fine_tuned_model') if 'model.ckpt-' and '.meta' in ckpt]\n",
        "steps = [(int(re.findall('\\d+', ckpt_file)[0])) for ckpt_file in ckpt_files]\n",
        "last_step = np.argmax(steps)\n",
        "last_model = ckpt_files[last_step]\n",
        "\n",
        "#exporting graph for testing\n",
        "!python {obj_detection + '/export_inference_graph.py'} \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path={pipeline_fn} \\\n",
        "    --output_directory=Frozen_Inf \\\n",
        "    --trained_checkpoint_prefix=fine_tuned_model/$last_model \n",
        "    \n",
        "print('Finished exporting')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Gdrive/My Drive/Tensorflow/workspace/training_dino\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0714 02:49:27.368729 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0714 02:49:27.432149 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "W0714 02:49:27.479244 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/export_inference_graph.py:156: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0714 02:49:27.479815 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/export_inference_graph.py:139: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0714 02:49:27.487153 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/exporter.py:367: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0714 02:49:27.489395 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/exporter.py:110: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0714 02:49:27.523669 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/core/preprocessor.py:2515: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0714 02:49:27.553058 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/meta_architectures/ssd_meta_arch.py:566: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0714 02:49:30.489067 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/core/anchor_generator.py:149: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W0714 02:49:30.501826 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0714 02:49:30.501988 139945191004032 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:49:30.600927 139945191004032 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:49:30.713108 139945191004032 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:49:30.817034 139945191004032 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:49:30.915242 139945191004032 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0714 02:49:31.016951 139945191004032 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "W0714 02:49:31.456238 139945191004032 deprecation.py:323] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/core/post_processing.py:255: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0714 02:49:31.810217 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/exporter.py:246: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0714 02:49:31.810470 139945191004032 deprecation.py:323] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/exporter.py:348: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "W0714 02:49:31.813887 139945191004032 deprecation.py:323] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/exporter.py:504: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "W0714 02:49:31.814954 139945191004032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "168 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              0\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   name\n",
            "-account_type_regexes       _trainable_variables\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     params\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "param: Number of parameters (in the Variable).\n",
            "\n",
            "Profile:\n",
            "node name | # parameters\n",
            "_TFProfRoot (--/3.03m params)\n",
            "  BoxPredictor_0 (--/24.22k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor (--/6.92k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/weights (1x1x576x12, 6.91k/6.91k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor_depthwise (--/5.18k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "    BoxPredictor_0/ClassPredictor (--/6.92k params)\n",
            "      BoxPredictor_0/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_0/ClassPredictor/weights (1x1x576x12, 6.91k/6.91k params)\n",
            "    BoxPredictor_0/ClassPredictor_depthwise (--/5.18k params)\n",
            "      BoxPredictor_0/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_0/ClassPredictor_depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "  BoxPredictor_1 (--/84.53k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor (--/30.74k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/weights (1x1x1280x24, 30.72k/30.72k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor_depthwise (--/11.52k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x1280x1, 11.52k/11.52k params)\n",
            "    BoxPredictor_1/ClassPredictor (--/30.74k params)\n",
            "      BoxPredictor_1/ClassPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_1/ClassPredictor/weights (1x1x1280x24, 30.72k/30.72k params)\n",
            "    BoxPredictor_1/ClassPredictor_depthwise (--/11.52k params)\n",
            "      BoxPredictor_1/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_1/ClassPredictor_depthwise/depthwise_weights (3x3x1280x1, 11.52k/11.52k params)\n",
            "  BoxPredictor_2 (--/33.84k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor (--/12.31k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor_depthwise (--/4.61k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "    BoxPredictor_2/ClassPredictor (--/12.31k params)\n",
            "      BoxPredictor_2/ClassPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_2/ClassPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n",
            "    BoxPredictor_2/ClassPredictor_depthwise (--/4.61k params)\n",
            "      BoxPredictor_2/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_2/ClassPredictor_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "  BoxPredictor_3 (--/16.94k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "    BoxPredictor_3/ClassPredictor (--/6.17k params)\n",
            "      BoxPredictor_3/ClassPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_3/ClassPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_3/ClassPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_3/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_3/ClassPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "  BoxPredictor_4 (--/16.94k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "    BoxPredictor_4/ClassPredictor (--/6.17k params)\n",
            "      BoxPredictor_4/ClassPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_4/ClassPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_4/ClassPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_4/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_4/ClassPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "  BoxPredictor_5 (--/8.50k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor (--/3.10k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/weights (1x1x128x24, 3.07k/3.07k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor_depthwise (--/1.15k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "    BoxPredictor_5/ClassPredictor (--/3.10k params)\n",
            "      BoxPredictor_5/ClassPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_5/ClassPredictor/weights (1x1x128x24, 3.07k/3.07k params)\n",
            "    BoxPredictor_5/ClassPredictor_depthwise (--/1.15k params)\n",
            "      BoxPredictor_5/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_5/ClassPredictor_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "  FeatureExtractor (--/2.84m params)\n",
            "    FeatureExtractor/MobilenetV2 (--/2.84m params)\n",
            "      FeatureExtractor/MobilenetV2/Conv (--/864 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/weights (3x3x3x32, 864/864 params)\n",
            "      FeatureExtractor/MobilenetV2/Conv_1 (--/409.60k params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/weights (1x1x320x1280, 409.60k/409.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv (--/800 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/depthwise (--/288 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights (3x3x32x1, 288/288 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/project (--/512 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/weights (1x1x32x16, 512/512 params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_1 (--/4.70k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise (--/864 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights (3x3x96x1, 864/864 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/expand (--/1.54k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights (1x1x16x96, 1.54k/1.54k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/project (--/2.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights (1x1x96x24, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_10 (--/64.90k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/project (--/36.86k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights (1x1x384x96, 36.86k/36.86k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_11 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_12 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_13 (--/152.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/project (--/92.16k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights (1x1x576x160, 92.16k/92.16k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_14 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_15 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_16 (--/469.44k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/project (--/307.20k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights (1x1x960x320, 307.20k/307.20k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_2 (--/8.21k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/project (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights (1x1x144x24, 3.46k/3.46k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_3 (--/9.36k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/project (--/4.61k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights (1x1x144x32, 4.61k/4.61k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_4 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_5 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_6 (--/20.16k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/project (--/12.29k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights (1x1x192x64, 12.29k/12.29k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_7 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_8 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_9 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256 (--/327.68k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights (1x1x1280x256, 327.68k/327.68k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128 (--/65.54k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights (1x1x512x128, 65.54k/65.54k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights (1x1x256x128, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64 (--/16.38k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights (1x1x256x64, 16.38k/16.38k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512 (--/131.07k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights (1x1x256x512, 131.07k/131.07k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise (--/2.30k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights (1x1x128x256, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights (1x1x128x256, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128 (--/8.19k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights (1x1x64x128, 8.19k/8.19k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise (--/576 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights (3x3x64x1, 576/576 params)\n",
            "\n",
            "======================End of Report==========================\n",
            "168 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              1\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   float_ops\n",
            "-account_type_regexes       .*\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     float_ops\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
            "\n",
            "Profile:\n",
            "node name | # float_ops\n",
            "_TFProfRoot (--/17.63k flops)\n",
            "  MultipleGridAnchorGenerator/mul_19 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_20 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/add_2 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/sub (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_27 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_28 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/add_5 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/sub_1 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_21 (1.08k/1.08k flops)\n",
            "  MultipleGridAnchorGenerator/mul_29 (600/600 flops)\n",
            "  MultipleGridAnchorGenerator/sub_2 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/add_8 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_35 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_36 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_37 (150/150 flops)\n",
            "  MultipleGridAnchorGenerator/sub_3 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_44 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_43 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/add_11 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_45 (54/54 flops)\n",
            "  MultipleGridAnchorGenerator/add_14 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_51 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_52 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/sub_4 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_53 (24/24 flops)\n",
            "  MultipleGridAnchorGenerator/mul_18 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_17 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/add_1 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/add (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_60 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/sub_5 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_59 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/add_17 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/add_3 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/add_4 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_25 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_26 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_18 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_46 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_47 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_40 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_48 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_39 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_38 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_32 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_31 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_30 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_19 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_17 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_54 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_24 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_23 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_61 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_15 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_16 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_56 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_22 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_55 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/add_6 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/add_7 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_34 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_33 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_15 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/add_10 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_42 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_41 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/add_9 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_16 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/add_12 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/add_13 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/mul_49 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/mul_50 (2/2 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_2 (1/1 flops)\n",
            "  Preprocessor/map/while/add_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
            "  Preprocessor/map/while/add (1/1 flops)\n",
            "  Preprocessor/map/while/Less_1 (1/1 flops)\n",
            "  Preprocessor/map/while/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/add_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/add (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_23 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_7 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_18 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/Minimum (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_58 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_57 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_19 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_20 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_21 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_22 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/assert_equal/Equal (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_7 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_15 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_16 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_9 (1/1 flops)\n",
            "\n",
            "======================End of Report==========================\n",
            "W0714 02:49:33.089483 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/exporter.py:397: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2019-07-14 02:49:34.166063: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-07-14 02:49:34.207816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:34.208244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-14 02:49:34.208559: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-14 02:49:34.209856: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-14 02:49:34.210970: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-14 02:49:34.211339: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-14 02:49:34.212786: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-14 02:49:34.213856: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-14 02:49:34.217268: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-14 02:49:34.217454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:34.217886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:34.218271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-14 02:49:34.227742: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-07-14 02:49:34.227995: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2c33c00 executing computations on platform Host. Devices:\n",
            "2019-07-14 02:49:34.228025: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-07-14 02:49:34.352524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:34.353041: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2c352c0 executing computations on platform CUDA. Devices:\n",
            "2019-07-14 02:49:34.353070: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-07-14 02:49:34.353328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:34.353685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-14 02:49:34.353782: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-14 02:49:34.353805: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-14 02:49:34.353832: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-14 02:49:34.353852: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-14 02:49:34.353893: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-14 02:49:34.353916: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-14 02:49:34.353954: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-14 02:49:34.354067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:34.354552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:34.354889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-14 02:49:34.354959: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-14 02:49:34.355951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-14 02:49:34.355979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-14 02:49:34.355993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-14 02:49:34.356279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:34.356663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:34.357052: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-07-14 02:49:34.357095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0714 02:49:34.357762 139945191004032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0714 02:49:34.360192 139945191004032 saver.py:1280] Restoring parameters from fine_tuned_model/model.ckpt-20000\n",
            "2019-07-14 02:49:36.952567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:36.952999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-14 02:49:36.953085: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-14 02:49:36.953110: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-14 02:49:36.953139: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-14 02:49:36.953159: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-14 02:49:36.953180: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-14 02:49:36.953199: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-14 02:49:36.953220: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-14 02:49:36.953325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:36.953724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:36.954054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-14 02:49:36.954097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-14 02:49:36.954111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-14 02:49:36.954126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-14 02:49:36.954404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:36.954797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:36.955161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I0714 02:49:36.956881 139945191004032 saver.py:1280] Restoring parameters from fine_tuned_model/model.ckpt-20000\n",
            "W0714 02:49:37.648024 139945191004032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "W0714 02:49:37.648272 139945191004032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "I0714 02:49:38.000802 139945191004032 graph_util_impl.py:311] Froze 404 variables.\n",
            "I0714 02:49:38.072569 139945191004032 graph_util_impl.py:364] Converted 404 variables to const ops.\n",
            "2019-07-14 02:49:38.226071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:38.226508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-14 02:49:38.226587: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-14 02:49:38.226615: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-14 02:49:38.226652: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-14 02:49:38.226673: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-14 02:49:38.226697: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-14 02:49:38.226717: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-14 02:49:38.226737: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-14 02:49:38.226838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:38.227263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:38.227582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-14 02:49:38.227630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-14 02:49:38.227644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-14 02:49:38.227654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-14 02:49:38.227945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:38.228342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-14 02:49:38.229262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0714 02:49:38.806321 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/exporter.py:274: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "W0714 02:49:38.808799 139945191004032 deprecation.py:323] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/exporter.py:277: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0714 02:49:38.809287 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/exporter.py:283: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "W0714 02:49:38.809422 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/exporter.py:286: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "W0714 02:49:38.809596 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/exporter.py:291: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "W0714 02:49:38.809721 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/exporter.py:293: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "I0714 02:49:38.809980 139945191004032 builder_impl.py:636] No assets to save.\n",
            "I0714 02:49:38.810056 139945191004032 builder_impl.py:456] No assets to write.\n",
            "I0714 02:49:39.102360 139945191004032 builder_impl.py:421] SavedModel written to: Frozen_Inf/saved_model/saved_model.pb\n",
            "W0714 02:49:39.136312 139945191004032 deprecation_wrapper.py:119] From /content/Gdrive/My Drive/Tensorflow/models/research/object_detection/utils/config_util.py:184: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "I0714 02:49:39.136513 139945191004032 config_util.py:186] Writing pipeline config file to Frozen_Inf/pipeline.config\n",
            "Finished exporting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ovEM7qiH4Sr",
        "colab_type": "code",
        "outputId": "9883065b-0c38-476f-d7e0-eaa36c7f1a5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_dino = '/content/Gdrive/My\\ Drive/Tensorflow/workspace/training_dino'\n",
        "%cd {training_dino}\n",
        "\n",
        "from google.colab import files\n",
        "files.download('label_map.pbtxt')\n",
        "files.download('Frozen_Inf/frozen_inference_graph.pb')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Gdrive/My Drive/Tensorflow/workspace/training_dino\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVMhUjnDzXgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}